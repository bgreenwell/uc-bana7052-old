---
title: "Inferences in Simple Linear Regression"
subtitle: "Lecture 02"
author: "Brandon M. Greenwell"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  xaringan::moon_reader:
    mathjax: "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_HTMLorMML"
    css: ["default", "metropolis", "metropolis-fonts", "hygge", "custom.css"]
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---
class: clear 

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE, servr.daemon = TRUE)

# Global chunk options
knitr::opts_chunk$set(
  echo = TRUE,
  dev = "png",
  fig.align = "center",
  fig.width = 6,
  fig.asp = 0.618,
  out.width = "70%",
  message = FALSE,
  warning = FALSE,
  error = FALSE
)

# Load required packages
library(dplyr)
library(ggplot2)
library(patchwork)
```

background-image: url(images/research-walberg-normal-distribution.jpg)


---

# Reading assignment

.larger[

* Chapter: 2

    - Sections: TBD

* Main topics: TBD
  
]


---

# Prerquisites

.scrollable[

```{r prerequisites, eval=FALSE}
# List of required (CRAN) packages
pkgs <- c(
  "animation",  # for pre-built statistical animations
  "dplyr",      # for data wrangling
  "ggplot2",    # for awesome graphics
  "HistData",   # for historical data sets
  "tibble"      # for nicer data frames
)

# Install required (CRAN) packages
for (pkg in pkgs) {
  if (!requireNamespace(pkg)) {  # check if already installed first
    install.packages(pkg)  # install it
  }
}

# Install additional (optional) awesomeness
install.packages(c("devtools", "magick"))
devtools::install_github("bgreenwell/roundhouse")
```

]


---

# Ready to begin?

--

```{r roundhouse-01}
roundhouse::kick("Chuck Norris counted to infinity, twice", 
                 width = 50)
```


---
class: clear 

background-image: url(images/significance.png)
background-size: 40%


---
class: clear, middle

```{r relationship-01, echo=FALSE, fig.width=6, fig.asp=0.618}
set.seed(101)
x <- rep(1:5, each = 10)
y <- 1 + 1*x + rnorm(length(x), sd = 3)
ggplot(data.frame(x, y), aes(x, y)) +
  geom_point() +
  labs(x = "X", y = "Y")
```


---
class: clear, middle

```{r relationship-02, echo=FALSE, fig.width=6, fig.asp=0.618}
set.seed(101)
x <- rep(1:5, each = 10)
y <- 1 + 1*x + rnorm(length(x), sd = 3)
ggplot(data.frame(x, y), aes(x, y)) +
  geom_point() +
  geom_smooth(method = "lm", formula = y ~ x, se = FALSE, col = "red2") +
  labs(x = "X", y = "Y")
```


---
class: clear, middle

```{r relationship-03, echo=FALSE, fig.width=6, fig.asp=0.618}
set.seed(101)
x <- rep(1:5, each = 10)
y <- 1 + 1*x + rnorm(length(x), sd = 3)
ggplot(data.frame(x, y), aes(x, y)) +
  geom_point() +
  geom_smooth(method = "lm", formula = y ~ x, se = TRUE, col = "red2") +
  labs(x = "X", y = "Y")
```


---

# Inferences concerning $\beta_1$

.large[

* **Bad:** Is there a relationship between $X$ and $Y$? (.red[not testable])

]

--

.large[

* **Good:** Is there a statistically significant linear relationship between $X$ and $Y$ at the $\alpha = 0.05$ level? (.green[testable])

]

--

.large[

* How can we reformulate this as a statistical test?

]

--

.large[

$$
H_0: \beta_1 = 0 \quad vs \quad H_1: \beta_1 \ne 0
$$

]

--

.large[

* Need a point estimate, test statistic, reference distribution, etc.

]


---

# Properties of $\widehat{\beta}_1$

.large[

* Recall from the previous lecture that LS estimation provides the best linear unbiased estimates .blue[(BLUE)] of $\beta_0$ and $\beta_1$; namely, $\widehat{\beta}_0$ and $\widehat{\beta}_1$

    - Unbiased since $E\left[\widehat{\beta}_0\right] = \beta_0$ and $E\left[\widehat{\beta}_1\right] = \beta_1$

    - Best in the sense that $\widehat{\beta}_0$ and $\widehat{\beta}_1$ have the smallest .purple[variance] among all other **linear unbiased** estimators of $\beta_0$ and $\beta_1$, respectively

]

--

.large[

* So what is $Var\left[\widehat{\beta}_0\right]$ and $Var\left[\widehat{\beta}_1\right]$?

]


---

# Properties of $\widehat{\beta}_1$

.large[

* Recall that the LS estimate of the slope is a weighted average of the (observed) response values: $\widehat{\beta}_1 = \sum_{i=1}^n w_iY_i$ 

]

--

.large[

* Since the $Y_i$ are independent, it follows that 

.small[

$$Var\left(\widehat{\beta}_1\right) = Var\left(\sum_{i=1}^n w_iY_i\right) = \sum_{i=1}^n w_i^2Var\left(Y_i\right) = \dots = \sigma^2 / S_{xx}$$

]

]


---

# Sampling distribution of $\widehat{\beta}_1$

.large[

* Assuming $\epsilon_i \stackrel{iid}{\sim} N\left(0, \sigma^2\right)$, then $\widehat{\beta}_1 \sim ???$ `r emo::ji("thinking")`

]

--
 
.large[

* $\widehat{\beta}_1 \sim N\left(\beta_1, \sigma^2/S_{xx}\right)$

]

--

.large[

* But we generally don't know $\sigma^2$, so how do we estimate it?

]

--

.large[

* Replace $\sigma^2$ with its point estimate ($\widehat{\sigma}^2 = MSE$), and the sampling distribution of $\widehat{\beta}_1$ becomes $\widehat{\beta}_1 \stackrel{\cdot}{\sim} N\left(\beta_1, \widehat{\sigma}^2/S_{xx}\right)$

]


---

# Standard errors

.large[

* .purple[The standard deviation of an estimate is referred to as its *standard error*]. For example,

$$\sqrt{Var\left(\widehat{\beta}_1\right)} = SE\left(\widehat{\beta}_1\right) = \sigma/\sqrt{S_{xx}}$$

]

--

.large[

* Since we don't know $\sigma^2$, we estimate $SE\left(\widehat{\beta}_1\right) = \sigma/\sqrt{S_{xx}}$ with its *plug-in* estimate

$$\widehat{SE}\left(\widehat{\beta}_1\right) = \widehat{\sigma}/\sqrt{S_{xx}}$$

]


---

# Inference regarding $\beta_1$

* Hypothesis test: $H_0: \beta_1 = c \quad vs \quad H_1: \beta_1 \ne c$

--

* Test statistic: $$t_{obs} = \frac{\widehat{\beta}_1 - c}{\widehat{SE}\left(\widehat{\beta}_1\right)} = \frac{\widehat{\beta}_1 - c}{\widehat{\sigma} / \sqrt{S_{xx}}}$$

--

* Rejection $H_0$ whenever $\left|t_{obs}\right| \ge t_{n - 2, 1 - \alpha/2}$

    - In R, $t$ quantiles can be obtained using `qt(1 - alpha/2, df = n-2)`, for example
    
```{r qt}
alpha <- 0.05           # significance level
n <- 30                 # sample size         
qt(1 - alpha/2, n - 2)  # cutoff value        #<<
```

--

* A $\left(1-\alpha\right)$ 100% confidence interval for $\beta_1$ is given by $\widehat{\beta}_1 \pm t_{n - 2, 1 - \alpha/2}\widehat{\sigma}/S_{xx}$


---

# Rocket propellant example

.scrollable[

```{r 02-rocket-01}
# Load the rocket propellant data
rocket <- read.csv("https://bgreenwell.github.io/uc-bana7052/data/rocket.csv")

# Fit an SLR model
rocket_fit <- lm(strength ~ age, data = rocket)

# Plot the data with the fitted mean response
investr::plotFit(rocket_fit)

# Print a summary of the fitted model
summary(rocket_fit)

# Compute a 95% CI for the slope
confint(rocket_fit, level = 0.95)  #<<
```

]


---
class: clear, middle

.large[

Can you interpret the confidence interval for $\beta_1$ in the previous example?

]

--

```{r 02-rocket-02}
confint(rocket_fit, level = 0.95)
```

With 95% confidence, we estimate that the mean strength of rockets decreases between 31.08 psi and 43.22 psi for every one-week increase in age.


---

# Your turn `r emo::ji("scream")`

.huge[

Fit an SLR model to the crystal weight data using `weight` as the response and `time` as the predictor. Find a 95% confidence interval for the slope and interpret the results in **plain english**.

]


---

# Solution `r set.seed(205); emo::ji("raised")`

.scrollable[

```{r 01-crystal-solution-01}
# Load the crystal weight data
data(crystal, package = "investr")

# Fit an SLR model
crystal_fit <- lm(weight ~ time, data = crystal)

# Plot the data with the fitted mean response
investr::plotFit(crystal_fit)

# Print a summary of the model
summary(crystal_fit)

# Compute a 95% CI for the regression coefficients
confint(crystal_fit, level = 0.95)  #<<
```

With 95% confidence, we estimate that the average weight of crystals increases between 0.43 grams and 0.58 grams for every one-hour increase in growth time.

]


---

# Rocket propellant example

.scrollable[

Using the rocket propellant example, test whether the slope significantly differes from $-40$ psi/week at the $\alpha = 0.05$ level.

```{r 02-rocket-03}
# Extract summary of estimated slope
(slope <- summary(rocket_fit)$coef["age", ])  #<<

# Compute test statistic
(t_obs <- (slope["Estimate"] + 40) / slope["Std. Error"])  # <<

# Compute cutoff from reference distribution
alpha <- 0.05
n <- nrow(rocket)
(t_ref <- qt(1 - alpha/2, df = n - 2))  #<<

# Decision rule
if (abs(t_obs) > t_ref) "reject H0" else "fail to reject H0"
```

]


---

# Your turn `r emo::ji("scream")`

.large[

Using the crystal weight example, test whether the slope significantly differes from $3/4$ grams/hour at the $\alpha = 0.1$ level.

]


---

# Solution `r set.seed(205); emo::ji("raised")`

.large[

$$H_0: \beta_1 = 3/4 \quad vs \quad H_1: \beta_1 \ne 3/4$$

```{r 01-crystal-solution-02}
# Compute a 90% CI for the slope
confint(crystal_fit, parm = "time", level = 0.9)
```

Since 3/4 lies outside of the 90% confidence interval for $\beta_1$, we reject the null hypoethsis at the 0.1 level and conclude that the slope significantly differs from 3/4. 

]


---

# Computing the *p*-value

.scrollable[

* One-sided test: $$p = Pr\left(T_{n-2} > \left|t_{obs}\right|\right)$$

* Two-sided test: $$p = 2 \times Pr\left(T_{n-2} > \left|t_{obs}\right|\right)$$

```{r p-value}
# From the rocket propellant example. What test does this correspond to?  #<<
(t_obs <- slope["Estimate"] / slope["Std. Error"])  #<<
(p_val <- 2 * pt(abs(t_obs), df = nrow(rocket) - 2, lower.tail = FALSE))  #<<
```

]


---

# Your turn `r emo::ji("scream")`

.large[

Compute the *p*-value for the previous test in the crystal weight example. What is your decision?

]


---

# Solution `r set.seed(205); emo::ji("raised")`

.medium[

$$H_0: \beta_1 = 3/4 \quad vs \quad H_1: \beta_1 \ne 3/4$$

```{r 01-crystal-solution-03}
slope <- summary(crystal_fit)$coef["time", ]
(t_obs <- (slope["Estimate"] - 3/4) / slope["Std. Error"])  #<<
(p_val <- 2 * pt(abs(t_obs), df = nrow(crystal) - 2, lower.tail = FALSE))  #<<
```

Since $p < 0.1$, we reject the null hypothesis and conclude that the slope significantly differs from 3/4. 

]


---

# Inferences concerning $\beta_0$

* Similar results exist for the intercept, just replace $\widehat{SE}\left(\widehat{\beta}_1\right)$ with $$\widehat{SE}\left(\widehat{\beta}_0\right) = MSE\left(\frac{1}{n} + \frac{\bar{X}^2}{S_{xx}}\right)$$


---
class: clear

.larger[

Consider the following hypotheses for the SLR model: $$H_0: \beta_1 = 0 \quad vs \quad H_1: \beta_1 \ne 0$$

]

--

.larger[

.center[

.red[

What does failing to reject $H_0$ imply about the relationship between $X$ and $Y$?

]

]

]


---

# ANOVA approach

.large[

* What does ANOVA refer to? `r emo::ji("thinking")`

]

--

.large[

* Partitioning sums of squares (SS)

    - Total SS: $SST = SS_{tot} = \sum_{i=1}^n\left(Y_i - \bar{Y}\right)^2$
    
    - Error SS: $SSE = SS_{err} = \sum_{i=1}^n\left(Y_i - \widehat{Y}_i\right)^2$
    
    - Regression SS: $SSR = SS_{reg} = \sum_{i=1}^n\left(\widehat{Y}_i - \bar{Y}\right)^2$
    
]


---

# ANOVA approach

.large[

* $\left(Y_i - \bar{Y}\right) = \left(\widehat{Y}_i - \bar{Y}\right) + \left(Y_i - \widehat{Y}_i\right)$

]

-- 

.large[

* It is easy to show that the sums of these squared deviations have the same relationship: $$\sum_{i=1}^n\left(Y_i - \bar{Y}\right) = \sum_{i=1}^n\left(\widehat{Y}_i - \bar{Y}\right) + \sum_{i=1}^n\left(Y_i - \widehat{Y}_i\right)$$

    - In other words, $SST = SSS + SSE$ (.purple[much like in a one-way ANOVA])

]


---

# ANOVA approach

.larger[

* Diving an SS by its associated *degrees of freedome* (df) produces mean squares (.purple[kind of like a standard deviation])

]

--

.larger[

* $MSR = \frac{SSR}{1}$

* $MSE = \frac{SSE}{1}$

]


---
class: clear, inverse, middle, center

.huge[

Is there a (linear) relationship between $X$ and $Y$? $$H_0: \beta_1 = 0 \quad vs \quad H_1: \beta_1 \ne 0$$

]


---

# ANOVA approach

.large[

* Test statistic: $$F_{obs} = \frac{MSR}{MSE}$$ with 1 .darkorange[numerator degrees of freedom] and $n-2$ .darkorange[denominator degrees of freedom]

]

--

.large[

* Reject $H_0$ at the $\alpha$ level whenever $F_{obs} > F_{1-\alpha, 1, n-2}$

    - Notice the use of $\alpha$ as opposed to $\alpha/2$ `r emo::ji("thinking")`
    
    - Is a large value of $F_{obs}$ good or bad?

]


---

# Rocket propellant example

.scrollable[

```{r 02-rocket-04}
# Compute ANOVA table for the fitted model
anova(rocket_fit)  #<<

# Print summary of fitted model
summary(rocket_fit)

# Fit an intercept only model
rocket_fit_reduced <- lm(strength ~ 1, data = rocket)
mean(rocket$strength)  # compare to estimated intercept  #<<
anova(rocket_fit_reduced, rocket_fit)  # compare models  #<<
```

]

---

# Your turn `r emo::ji("scream")`

.larger[

Using the crystal weight example, use an $F$-test to test whether or not there is a relationship between `time` and `weight` at the $\alpha = 0.05$ level. Manually compute the *p*-value for this test and `r emo::ji("pray")` that it matches the output from `summary()`.

]


---

# Solution `r set.seed(205); emo::ji("raised")`

.scrollable[

```{r 01-crystal-solution-04}
# Print summary of the fitted model
summary(crystal_fit)

# What values can we pull out from summary()
names(summary(crystal_fit))

# Observed test statitic
f_obs <- summary(crystal_fit)$fstatistic

# Compute p-value (one approach)
pf(f_obs, df1 = 1, df2 = nrow(crystal) - 2, lower.tail = FALSE)

# Compute p-value (another approach)
1 - pf(f_obs, df1 = 1, df2 = nrow(crystal) - 2)
```

]
