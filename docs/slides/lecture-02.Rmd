---
title: "Inferences in Simple Linear Regression"
subtitle: "Lecture 02"
author: "Brandon M. Greenwell"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  xaringan::moon_reader:
    mathjax: "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_HTMLorMML"
    css: ["default", "metropolis", "metropolis-fonts", "hygge", "custom.css"]
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---
class: clear 

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE, servr.daemon = TRUE)

# Global chunk options
knitr::opts_chunk$set(
  echo = TRUE,
  dev = "png",
  fig.align = "center",
  fig.width = 6,
  fig.asp = 0.618,
  out.width = "70%",
  message = FALSE,
  warning = FALSE,
  error = FALSE
)

# Load required packages
library(dplyr)
library(ggplot2)
library(patchwork)
```

background-image: url(images/research-walberg-normal-distribution.jpg)


---

# Reading assignment

.larger[

* Chapter: 2

    - Sections: TBD

* Main topics: TBD
  
]


---

# Prerquisites

.scrollable[

```{r prerequisites, eval=FALSE}
# List of required (CRAN) packages
pkgs <- c(
  "animation",  # for pre-built statistical animations
  "dplyr",      # for data wrangling
  "ggplot2",    # for awesome graphics
  "HistData",   # for historical data sets
  "tibble"      # for nicer data frames
)

# Install required (CRAN) packages
for (pkg in pkgs) {
  if (!requireNamespace(pkg)) {  # check if already installed first
    install.packages(pkg)  # install it
  }
}

# Install additional (optional) awesomeness
install.packages(c("devtools", "magick"))
devtools::install_github("bgreenwell/roundhouse")
```

]


---

# Ready to begin?

--

```{r roundhouse-01}
roundhouse::kick("Chuck Norris counted to infinity, twice", 
                 width = 50)
```


---
class: clear 

background-image: url(images/significance.png)
background-size: 40%


---
class: clear, middle

```{r relationship-01, echo=FALSE, fig.width=6, fig.asp=0.618}
set.seed(101)
x <- rep(1:5, each = 10)
y <- 1 + 1*x + rnorm(length(x), sd = 3)
ggplot(data.frame(x, y), aes(x, y)) +
  geom_point() +
  labs(x = "X", y = "Y")
```


---
class: clear, middle

```{r relationship-02, echo=FALSE, fig.width=6, fig.asp=0.618}
set.seed(101)
x <- rep(1:5, each = 10)
y <- 1 + 1*x + rnorm(length(x), sd = 3)
ggplot(data.frame(x, y), aes(x, y)) +
  geom_point() +
  geom_smooth(method = "lm", formula = y ~ x, se = FALSE, col = "red2") +
  labs(x = "X", y = "Y")
```


---
class: clear, middle

```{r relationship-03, echo=FALSE, fig.width=6, fig.asp=0.618}
set.seed(101)
x <- rep(1:5, each = 10)
y <- 1 + 1*x + rnorm(length(x), sd = 3)
ggplot(data.frame(x, y), aes(x, y)) +
  geom_point() +
  geom_smooth(method = "lm", formula = y ~ x, se = TRUE, col = "red2") +
  labs(x = "X", y = "Y")
```


---

# Inferences concerning $\beta_1$

.large[

* **Bad:** Is there a relationship between $X$ and $Y$? (.red[not testable])

]

--

.large[

* **Good:** Is there a statistically significant linear relationship between $X$ and $Y$ at the $\alpha = 0.05$ level? (.green[testable])

]

--

.large[

* How can we reformulate this as a statistical test?

]

--

.large[

$$
H_0: \beta_1 = 0 \quad vs \quad H_1: \beta_1 \ne 0
$$

]

--

.large[

* Need a point estimate, test statistic, reference distribution, etc.

]


---

# Properties of $\widehat{\beta}_1$

.large[

* Recall from the previous lecture that LS estimation provides the best linear unbiased estimates .blue[(BLUE)] of $\beta_0$ and $\beta_1$; namely, $\widehat{\beta}_0$ and $\widehat{\beta}_1$

    - Unbiased since $E\left[\widehat{\beta}_0\right] = \beta_0$ and $E\left[\widehat{\beta}_1\right] = \beta_1$

    - Best in the sense that $\widehat{\beta}_0$ and $\widehat{\beta}_1$ have the smallest .purple[variance] among all other **linear unbiased** estimators of $\beta_0$ and $\beta_1$, respectively

]

--

.large[

* So what is $Var\left[\widehat{\beta}_0\right]$ and $Var\left[\widehat{\beta}_1\right]$?

]


---

# Properties of $\widehat{\beta}_1$

.large[

* Recall that the LS estimate of the slope is a weighted average of the (observed) response values: $\widehat{\beta}_1 = \sum_{i=1}^n w_iY_i$ 

]

--

.large[

* Since the $Y_i$ are independent, it follows that 

.small[

$$Var\left(\widehat{\beta}_1\right) = Var\left(\sum_{i=1}^n w_iY_i\right) = \sum_{i=1}^n w_i^2Var\left(Y_i\right) = \dots = \sigma^2 / S_{xx}$$

]

]


---

# Sampling distribution of $\widehat{\beta}_1$

.large[

* Assuming $\epsilon_i \stackrel{iid}{\sim} N\left(0, \sigma^2\right)$, then $\widehat{\beta}_1 \sim ???$ `r emo::ji("thinking")`

]

--
 
.large[

* $\widehat{\beta}_1 \sim N\left(\beta_1, \sigma^2/S_{xx}\right)$

]

--

.large[

* But we generally don't know $\sigma^2$, so how do we estimate it?

]

--

.large[

* Replace $\sigma^2$ with its point estimate ($\widehat{\sigma}^2 = MSE$), and the sampling distribution of $\widehat{\beta}_1$ becomes $\widehat{\beta}_1 \stackrel{\cdot}{\sim} N\left(\beta_1, \widehat{\sigma}^2/S_{xx}\right)$

]


---

# Standard errors

.large[

* .purple[The standard deviation of an estimate is referred to as its *standard error*]. For example,

$$\sqrt{Var\left(\widehat{\beta}_1\right)} = SE\left(\widehat{\beta}_1\right) = \sigma/\sqrt{S_{xx}}$$

]

--

.large[

* Since we don't know $\sigma^2$, we estimate $SE\left(\widehat{\beta}_1\right) = \sigma/\sqrt{S_{xx}}$ with its *plug-in* estimate

$$\widehat{SE}\left(\widehat{\beta}_1\right) = \widehat{\sigma}/\sqrt{S_{xx}}$$

]


---

# Inference regarding $\beta_1$

* Hypothesis test: $H_0: \beta_1 = c \quad vs \quad H_1: \beta_1 \ne c$

--

* Test statistic: $$t_{obs} = \frac{\widehat{\beta}_1 - c}{\widehat{SE}\left(\widehat{\beta}_1\right)} = \frac{\widehat{\beta}_1 - c}{\widehat{\sigma} / \sqrt{S_{xx}}}$$

--

* Rejection $H_0$ whenever $\left|t_{obs}\right| \ge t_{n - 2, 1 - \alpha/2}$

    - In R, $t$ quantiles can be obtained using `qt(1 - alpha/2, df = n-2)`, for example
    
```{r qt}
alpha <- 0.05           # significance level
n <- 30                 # sample size         
qt(1 - alpha/2, n - 2)  # cutoff value        #<<
```

--

* A $\left(1-\alpha\right)$ 100% confidence interval for $\beta_1$ is given by $\widehat{\beta}_1 \pm t_{n - 2, 1 - \alpha/2}\widehat{\sigma}/S_{xx}$


---

# Rocket propellant example

.scrollable[

```{r rocket-01}
# Load the rocket propellant data
rocket <- read.csv("https://bgreenwell.github.io/uc-bana7052/data/rocket.csv")

# Fit an SLR model
rocket_fit <- lm(strength ~ age, data = rocket)

# Plot the data with the fitted mean response
investr::plotFit(rocket_fit)

# Print a summary of the fitted model
summary(rocket_fit)

# Compute a 95% CI for the slope
confint(rocket_fit, level = 0.95)  #<<
```

]


---
class: clear, middle

.large[

Can you interpret the confidence interval for $\beta_1$ in the previous example?

]

--

```{r rocket-02}
confint(rocket_fit, level = 0.95)
```

With 95% confidence, we estimate that the mean strength of rockets decreases between 31.08 psi and 43.22 psi for every one-week increase in age.


---

# Your turn `r emo::ji("scream")`

.huge[

Fit an SLR model to the crystal weight data using `weight` as the response and `time` as the predictor. Find a 95% confidence interval for the slope and interpret the results in **plain english**.

]


---

# Solution `r set.seed(205); emo::ji("raised")`

.scrollable[

```{r 01-crystal-solution-01}
# Load the crystal weight data
data(crystal, package = "investr")

# Fit an SLR model
crystal_fit <- lm(weight ~ time, data = crystal)

# Plot the data with the fitted mean response
investr::plotFit(crystal_fit)

# Print a summary of the model
summary(crystal_fit)

# Compute a 95% CI for the regression coefficients
confint(crystal_fit, level = 0.95)  #<<
```

With 95% confidence, we estimate that the average weight of crystals increases between 0.43 grams and 0.58 grams for every one-hour increase in growth time.

]


---

# Rocket propellant example

.scrollable[

Using the rocket propellant example, test whether the slope significantly differs from $-40$ psi/week at the $\alpha = 0.05$ level.

```{r rocket-03}
# Extract summary of estimated slope
(slope <- summary(rocket_fit)$coef["age", ])  #<<

# Compute test statistic
(t_obs <- (slope["Estimate"] + 40) / slope["Std. Error"])  # <<

# Compute cutoff from reference distribution
alpha <- 0.05
n <- nrow(rocket)
(t_ref <- qt(1 - alpha/2, df = n - 2))  #<<

# Decision rule
if (abs(t_obs) > t_ref) "reject H0" else "fail to reject H0"
```

]


---

# Your turn `r emo::ji("scream")`

.large[

Using the crystal weight example, test whether the slope significantly differs from $3/4$ grams/hour at the $\alpha = 0.1$ level.

]


---

# Solution `r set.seed(205); emo::ji("raised")`

.large[

$$H_0: \beta_1 = 3/4 \quad vs \quad H_1: \beta_1 \ne 3/4$$

```{r 01-crystal-solution-02}
# Compute a 90% CI for the slope
confint(crystal_fit, parm = "time", level = 0.9)
```

Since 3/4 lies outside of the 90% confidence interval for $\beta_1$, we reject the null hypothesis at the 0.1 level and conclude that the slope significantly differs from 3/4. 

]


---

# Computing the *p*-value

.scrollable[

* One-sided test: $$p = Pr\left(T_{n-2} > \left|t_{obs}\right|\right)$$

* Two-sided test: $$p = 2 \times Pr\left(T_{n-2} > \left|t_{obs}\right|\right)$$

```{r p-value}
# From the rocket propellant example. What test does this correspond to?  #<<
(t_obs <- slope["Estimate"] / slope["Std. Error"])  #<<
(p_val <- 2 * pt(abs(t_obs), df = nrow(rocket) - 2, lower.tail = FALSE))  #<<
```

]


---

# Your turn `r emo::ji("scream")`

.large[

Compute the *p*-value for the previous test in the crystal weight example. What is your decision?

]


---

# Solution `r set.seed(205); emo::ji("raised")`

.medium[

$$H_0: \beta_1 = 3/4 \quad vs \quad H_1: \beta_1 \ne 3/4$$

```{r 01-crystal-solution-03}
slope <- summary(crystal_fit)$coef["time", ]
(t_obs <- (slope["Estimate"] - 3/4) / slope["Std. Error"])  #<<
(p_val <- 2 * pt(abs(t_obs), df = nrow(crystal) - 2, lower.tail = FALSE))  #<<
```

Since $p < 0.1$, we reject the null hypothesis and conclude that the slope significantly differs from 3/4. 

]


---

# Inferences concerning $\beta_0$

* Similar results exist for the intercept, just replace $\widehat{SE}\left(\widehat{\beta}_1\right)$ with $$\widehat{SE}\left(\widehat{\beta}_0\right) = MSE\left(\frac{1}{n} + \frac{\bar{X}^2}{S_{xx}}\right)$$


---
class: clear

.larger[

Consider the following hypotheses for the SLR model: $$H_0: \beta_1 = 0 \quad vs \quad H_1: \beta_1 \ne 0$$

]

--

.larger[

.center[

.red[

What does failing to reject $H_0$ imply about the relationship between $X$ and $Y$?

]

]

]


---

# ANOVA approach

.large[

* What does ANOVA refer to? `r emo::ji("thinking")`

]

--

.large[

* Partitioning sums of squares (SS)

    - Total SS: $SST = SS_{tot} = \sum_{i=1}^n\left(Y_i - \bar{Y}\right)^2$
    
    - Error SS: $SSE = SS_{err} = \sum_{i=1}^n\left(Y_i - \widehat{Y}_i\right)^2$
    
    - Regression SS: $SSR = SS_{reg} = \sum_{i=1}^n\left(\widehat{Y}_i - \bar{Y}\right)^2$
    
]


---

# ANOVA approach

.large[

* $\left(Y_i - \bar{Y}\right) = \left(\widehat{Y}_i - \bar{Y}\right) + \left(Y_i - \widehat{Y}_i\right)$

]

-- 

.large[

* It is easy to show that the sums of these squared deviations have the same relationship: $$\sum_{i=1}^n\left(Y_i - \bar{Y}\right) = \sum_{i=1}^n\left(\widehat{Y}_i - \bar{Y}\right) + \sum_{i=1}^n\left(Y_i - \widehat{Y}_i\right)$$

    - In other words, $SST = SSS + SSE$ (.purple[much like in a one-way ANOVA])

]


---

# ANOVA approach

.larger[

* Diving an SS by its associated *degrees of freedom* (df) produces mean squares (.purple[kind of like a standard deviation])

]

--

.larger[

* $MSR = \frac{SSR}{1}$

* $MSE = \frac{SSE}{1}$

]


---
class: clear, inverse, middle, center

.huge[

Is there a (linear) relationship between $X$ and $Y$? $$H_0: \beta_1 = 0 \quad vs \quad H_1: \beta_1 \ne 0$$

]


---

# ANOVA approach

.large[

* Test statistic: $$F_{obs} = \frac{MSR}{MSE}$$ with 1 .darkorange[numerator degrees of freedom] and $n-2$ .darkorange[denominator degrees of freedom]

]

--

.large[

* Reject $H_0$ at the $\alpha$ level whenever $F_{obs} > F_{1-\alpha, 1, n-2}$

    - Notice the use of $\alpha$ as opposed to $\alpha/2$ `r emo::ji("thinking")`
    
    - Is a large value of $F_{obs}$ good or bad?

]


---

# Rocket propellant example

.scrollable[

```{r rocket-04}
# Compute ANOVA table for the fitted model
anova(rocket_fit)  #<<

# Print summary of fitted model
summary(rocket_fit)
```

]

---

# Your turn `r emo::ji("scream")`

.larger[

Using the crystal weight example, use an *F*-test to test whether or not there is a relationship between `time` and `weight` at the $\alpha = 0.05$ level. Manually compute the *p*-value for this test and `r emo::ji("pray")` that it matches the output from `summary()`.

]


---

# Solution `r set.seed(205); emo::ji("raised")`

.scrollable[

```{r 01-crystal-solution-04}
# Print summary of the fitted model
summary(crystal_fit)

# What values can we pull out from summary()
names(summary(crystal_fit))

# Observed test statitic
f_obs <- summary(crystal_fit)$fstatistic

# Compute p-value (one approach)
pf(f_obs, df1 = 1, df2 = nrow(crystal) - 2, lower.tail = FALSE)

# Compute p-value (another approach)
1 - pf(f_obs, df1 = 1, df2 = nrow(crystal) - 2)
```

]


---

# F or t?

.large[

* For every two-sided *t*-test, there is a corresponding *F*-test

    - $t_{obs}^2 = F_{obs}$

]

--

.large[

* In the SLR model, these the approaches are equivalent 

    - The *F*-test becomes useful when we start adding more predictors (i.e., in multiple linear regression)

]


---

# The general linear test

.large[

* Full model: $Y_i = \beta_0 + \beta_1 X_i + \epsilon_i$

]

--

.large[

* .magenta[Reduced] model: $Y_i = \beta_0 + \epsilon_i$

]

--

.large[

* Implied test: $H_0: \beta_1 = 0 \quad vs \quad H_1: \beta_1 \ne 0$

    - $F_{obs} = \frac{SSE(R) - SSE(F)}{df_R - df_F} \div \frac{SSE(F)}{df_F}$
    
    - Reject $H_0$ whenever $F_{obs} > F_{1 - \alpha, df_R - df_F, df_F}$
    
    - More useful in multiple linear regression, but we'll introduce it here!

]


---

# Rocket propellant example

.scrollable[

```{r rocket-05}
# Fit an intercept only model
rocket_fit_reduced <- lm(strength ~ 1, data = rocket)
mean(rocket$strength)  # compare to estimated intercept  #<<
anova(rocket_fit_reduced, rocket_fit)  # compare models  #<<
```

]


---

# Rule of thumb `r set.seed(101); emo::ji("thumb")`

.large[

Using 

$$Esimtate \pm 2 \times SE$$

for an approximate 95% CI is incredibly robust!

```{r robust}
sapply(c(10, 20, 30, 50, Inf), function(x)  #<<
  qt(0.975, df = x))                        #<<
```

]


---

# Estimation versus prediction

* Regression models are often used to predict a new response or estimate a mean
response for a given value of the predictor $X$

* We have seen how to compute a predicted value $\widehat{Y} = \widehat{\beta}_0 + \widehat{\beta}_1 X$

     - However, as with any estimate, we need a measure of reliability associated with $\widehat{Y}_0$.

* The fitted regression line for the `r emo::ji("rocket")` example is given by $$\widehat{Y} = 2627.822 - 37.154 X$$ where $X$ is the age in weeks of the rocket and $Y$ is its psi strength

* Suppose we want to predict the strength of a new rocket at an age of 15 weeks. Then we would simply plug $X = 15$ into the estimated regression equation to get a predicted value of $\widehat{Y} = 2627.822 âˆ’ 37.154(15) = 2070.512$ (psi).


---

# Estimation versus prediction `r emo::ji("confused")`

* Regression analysis is really a problem of estimating a conditional mean or expectation, for example, in SLR we have $$E\left(Y|X\right) = \beta_0 + \beta_1 X$$

* Here, $E\left(Y|X\right)$ corresponds to the average value of the response $Y$ for all
units in the population with a specific $X$ value

* Suppose we want to estimate the mean strength of .bold[all rockets] with an age of 15 weeks

    - For the prediction problem, we want to predict the strength of a **single rocket** at 15 weeks
    
    - For the estimation problem, we want to estimate the mean of the population of all rockets that are 15 weeks old 
    
    - In both cases, we use $\widehat{Y} = 2070.512$ as the predicted strength and as the estimate of the mean strength of of rockets that are 15 weeks old

--

.center[.small[.content-box-yellow[

In other words, the point estimate is the same for prediction and estimation, the difference lies in the estimated standard error of each!

]


---

# Estimation versus prediction

* There is more uncertainty associated with prediction a single new observation (**Why?** `r emo::ji("thinking")`)

* For a given value of $X$, it is customary to compute **confidence intervals for an estimated mean response** and a **prediction interval for a single new response value**

* The idea of a prediction interval is to determine an interval that will contain a certain percentage of the population

    - Because a prediction interval is attempting to capture a single, random future response, as opposed to the mean of the conditional population, it will be wider than the associated confidence interval

--

.center[.small[.content-box-yellow[

**My opinion:** confidence intervals for the mean response are far more common and useful in practice

]]]


---

# Anscombe's quartet

TBD


---

# CI for the mean response

.large[

* We can estimate the mean response given $X = X_0$ as $\widehat{Y}_0 = \widehat{\beta}_0 + \widehat{\beta}_1 X_0$ 

]

--

.large[

* How can we construct a CI for $\widehat{Y}$? `r emo::ji("thinking")`

]

--

.large[

* What is the sampling distribution of $\widehat{Y}_0$? `r emo::ji("thinking")`

]

--

.large[

For the normal error regression model $$\frac{\widehat{Y}_0 - E\left(\widehat{Y}_0\right)}{\widehat{SE}\left(\widehat{Y}_0\right)} \sim t_{n-2}$$

]


---

# CI for the mean response

.large[

* $E\left(\widehat{Y}_0\right) = \beta_0 + \beta_1 x_0$

* $\widehat{SE}\left(\widehat{Y}_0\right) = MSE\left[\frac{1}{n} + \frac{\left(X_0 - \bar{X}\right)^2}{S_{xx}}\right]$

]

--

.large[

A $1 - \alpha$ CI for the mean response at $X = X_0$ is given by $$\widehat{Y}_0 \pm t_{1 - \alpha/2, n-2} MSE\left[\frac{1}{n} + \frac{\left(X_0 - \bar{X}\right)^2}{S_{xx}}\right]$$

]

--

.medium[.center[.red[At what point is this interval smallest?]]]


---

# Rocket propellant example

Compute a 95% confidence interval for the mean response at 15 weeks

.scrollable[

```{r rocket-06}
# Confidence interval for the mean response at age = 15
new_data <- data.frame(age = 15)
predict(rocket_fit, newdata = new_data, interval = "confidence")

# Plot the (pointwise) confidence band around the fitted regression line
library(investr)  # for plotFit() function
plotFit(rocket_fit, interval = "confidence")
abline(v = 15, col = "red2")
plotFit(rocket_fit, interval = "confidence", shade = TRUE, xlim = c(-20, 100))
```

]


---

# Your turn `r emo::ji("scream")`

.larger[

Using the crystal weight example, compute a 90% confidence interval for the mean response at $X = 20$ hours. Plot a (pointwise) 90% confidence band using the `investr::plotFit()` function. See `?predict.lm` and `?investr::plotFit` for help.

]


---

# Solution `r set.seed(205); emo::ji("raised")`

.scrollable[

```{r 01-crystal-solution-05}
# Confidence interval for the mean response at age = 15
new_data <- data.frame(time = 20)
predict(crystal_fit, newdata = new_data, interval = "confidence",
        level = 0.9)  #<<

# Plot the (pointwise) confidence band around the fitted regression line
plotFit(crystal_fit, interval = "confidence", cex = 1.4, pch = 19, shade = TRUE,
        col.conf = adjustcolor("red2", alpha.f = 0.5))
```

]


---
class: clear, middle, center


.larger[[Confidence intervals vs prediction intervals](https://stats.stackexchange.com/questions/16493/difference-between-confidence-intervals-and-prediction-intervals)]


---

# Coefficient of determination

.large[

* A useful performance metric in linear regression, called the *coefficient of determination*, is defined as $$R^2 = \frac{SSR}{SST} = 1 - \frac{SSE}{SST}$$

]

--

.large[

* In linear regression, $R^2$ can be interpreted as the fraction of variance explained (FVE) (i.e., the proportion of the variation in $Y$ that can be explained by $X$)

    - $0 \le R^2 \le 1$

]


---

# Rocket propellant example

.scrollable[

```{r rocket-07}
# Print model summary
summary(rocket_fit)

# Extract R-squared from the model summary
summary(rocket_fit)$r.squared

# Compute R-squared by hand
anova(rocket_fit)
SSE <- anova(rocket_fit)["Residuals", "Sum Sq"]
SST <- sum((rocket$strength - mean(rocket$strength)) ^ 2)
1 - SSE/SST
```

]


---

# Your turn `r emo::ji("cry")`

.huge[

Compute the coefficient of determination (i.e., $R^2$) for the crystal weight example by hand .purple[and interpret its value].

]


---

# Solution `r set.seed(205); emo::ji("raised")`

.scrollable[

```{r 01-crystal-solution-06}
# Compute R-squared by hand
anova(crystal_fit)
SSE <- anova(crystal_fit)["Residuals", "Sum Sq"]
SST <- sum((crystal$weight - mean(crystal$weight)) ^ 2)
1 - SSE/SST
```

Roughly `r scales::percent(summary(crystal_fit)$r.squared)` of the variability in the final weight of crystals is explained by the growth time. (At least in this sample.)

]


---

# Common misunderstandings about $R^2$

.large[

* A high $R^2$ (i.e., near 1) indicates that a useful (i.e., accurate) prediction can be made

]

--

.large[

* A high $R^2$ (i.e., near 1) indicates that the estimated regression line provides a good fit to the data

]

--

.large[

* A small $R^2$ (i.e., near zero) indicates that $X$ and $Y$ are not related

]


---

# Other things to look out for `r emo::ji("eyes")`

.large[

* $R^2$ will .red[always] increase when more terms are added to the model (.orange[more on this in multiple linear regression])

]

--

.large[

* As the range of $X$ increases/decreases, $R^2$ also generally increases/decreases

]

--

.large[

* $R^2$ does not indicate the **appropriateness** of a linear model

]

--

.larger[.center[.blue[So why even use the coefficient of determination?]]]


---

# Coefficient of correlation

.larger[

In SLR, there is a connection between $R^2$ and the Pearson correlation coefficient between $X$ and $Y$: $$r = \pm \sqrt{R^2}$$

]

-- 

.large[.center[.blue[*r* will have the same sign as the estimated slope!]]]

---

# Rocket propellant example

.medium[

```{r rocket-08}
# Compute coefficient of correlation
(r_squared <- summary(rocket_fit)$r.squared)
sqrt(r_squared)
cor(rocket)  # compare with correlation coefficient
```

]
