---
title: "Residual Diagnostics and Remedial Measures"
subtitle: "Lecture 04"
author: "Brandon M. Greenwell"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  xaringan::moon_reader:
    mathjax: "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_HTMLorMML"
    css: [default, metropolis, metropolis-fonts, hygge, "custom.css"]
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---
class: clear, middle

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE, servr.daemon = TRUE)

# Global chunk options
knitr::opts_chunk$set(
  cache = FALSE,
  echo = TRUE,
  dev = "svglite",
  fig.align = "center",
  # fig.width = 6,
  # fig.asp = 0.618,
  # out.width = "70%",
  message = FALSE,
  warning = FALSE,
  error = FALSE
)

# Bitmoji id
my_id <- "1551b314-5e8a-4477-aca2-088c05963111-v1"

# Load required packages
library(ggplot2)
```

.large[

* Required reading

    - Chapters: 3, 6, and 10

    - Sections: 3.1-3.4, 3.7, 6.8, and 10.1-10.4

* Main topics:

    - Diagnostic and Remedial Measures (3.1-3.4, 3.7, 6.8,)
  
    - Model Adequacy and Outlying Observations (10.1-10.4)

]


---

# Prerquisites

.scrollable[

.medium[

```{r prerequisites, eval=FALSE}
# List of required (CRAN) packages
pkgs <- c(
  NULL
)

# Install required (CRAN) packages
for (pkg in pkgs) {
  if (!requireNamespace(pkg)) {  # check if already installed
    install.packages(pkg)  # install it
  }
}

# Install additional (optional) awesomeness
install.packages(c("devtools", "magick"))
devtools::install_github("bgreenwell/roundhouse")
```

]

]


---
class: clear, center, middle

```{r lets-go, echo=FALSE, out.width="70%"}
set.seed(4); RBitmoji::plot_comic(my_id, tag = "lets go")
```


---
class: clear, center, middle, inverse

.huger[

What are *residual diagnostics* and *remedial measures* and why do we need them?

]


---
class: clear, center, middle, inverse

.huger[

What's wrong with my model and how do I fix it?

]


---
class: clear 

.large[.bold[.center[.green[What are the typical assumptions of the linear regression model?]]]]

.large[

$$Y_i = \beta_0 + \beta_1 X_{i1} + \dots + \beta_{p-1} X_{i, p-1} + \epsilon_i\\i = 1, 2, \dots, n$$

]

--

.large[

* Independent observations: $Cov\left(\epsilon_i, \epsilon_j\right) = 0$ $\left(i \ne j\right)$

]

--

.large[

* Constant variance: $Var\left(\epsilon_i\right) = Cov\left(\epsilon_i, \epsilon_i\right) = \sigma^2$

]

--

.large[

* Normally distributed errors: $\epsilon_i \stackrel{iid}{\sim} N\left(0, \sigma^2\right)$ (.red[required for **traditional** statistical inference])

]

--

.large[

* We assume that we have the .bold[.darkorange[correct model]]!

]


---
class: clear, center, middle, inverse

.huger[

How do we know if the model is ~~wrong~~ bad?

]


---
class: clear, center, middle

.huger[What is a residual?]

--

.huge[ 

$e_i = Y_i - \widehat{Y}_i$ 

.red[observed error = data - fit]

]


---

# What can they tell us?

.medium[

* Plot of residuals against predictor variable (checking non-linearity).

* Plot of absolute or squared residuals against predictor variable (check non-constant variance)

* Plot of residuals against fitted values (non-constant variance and non-linearity)

* Plot of residuals against time or another sequence (non-independence)

* Plot of residuals against omitted predictor variables (missing potentially important predictors)

* Boxplot of residuals (outlying observations)

* Normal probability plot of residuals (non-normality).

]


---
class: clear, middle, center

```{r example-diagnostics, echo=FALSE, fig.width=6, fig.asp=1, out.width="70%"}
# Simualated data sets
set.seed(101)
n <- 100
df1 <- tibble::tibble(
  x = runif(n, min = -5, max = 5),
  y = 1 + 2*x^2 + rnorm(n, sd = 2)
)
df2 <- tibble::tibble(
  x = runif(n, min = 1, max = 10),
  y = 1 + 4*x + rnorm(n, sd = 2*x)
)
df2 <- rbind(df2, data.frame(x = 2, y = 50))
df3 <- tibble::tibble(
  x = runif(n, min = 1, max = 10),
  y = 1 + 4*x + arima.sim(list(order = c(1,0,0), ar = 0.7), n = n, sd = 20)
)
df4 <- tibble::tibble(
  x = runif(n, min = 1, max = 10),
  y = 1 + 4*x + rlnorm(n, sd = 1)
)

# Fitted models
fit1 <- lm(y ~ x, data = df1)
fit2 <- lm(y ~ x, data = df2)
fit3 <- lm(y ~ x, data = df3)
fit4 <- lm(y ~ x, data = df4)

# Residuals
r1 <- residuals(fit1)
r2 <- rstandard(fit2)
r3 <- residuals(fit3)
r4 <- residuals(fit4)

# Residual plots
par(mfrow = c(2, 2))
plot(df1$x, r1, xlab = "X", ylab = "Residual", 
     main = "Misspecified mean structure")
abline(h = 0, lty = "dotted", col = "red2")
plot(df2$x, r2, xlab = "X", ylab = "Residual",
     main = "Non-constant variance")
points(df2$x[101L], r2[101L], pch = 19, col = "red2")
abline(h = 0, lty = "dotted", col = "red2")
plot(residuals(fit3), xlab = "Index", ylab = "Residual", type = "b",
     main = "Serial correlation")
abline(h = 0, lty = "dotted", col = "red2")
qqnorm(r4, main = "Non-normal errors")
qqline(r4, lty = "dotted", col = "red2")
```


---

# Residual analysis

.large[

* Residuals help in identifying a misspecified mean structure (i.e., $\boldsymbol{\beta}\boldsymbol{X}$)

* *Scaled residuals* help in identifying outliers or extreme values

* Common types of residuals:

    - Standardized Residuals
    
    - Studentized residuals
    
    - PRESS Residuals 

]


---
class: clear, middle, center

.huge[Normal Q-Q plot can be used to asses the "normalityness" of a set of observations]


---
class: clear 

```{r qqnorm-normal, fig.width=6, fig.asp=0.618, out.width="70%"}
set.seed(101)
x <- rnorm(100)
qqnorm(x, main = "Normal data")
qqline(x, lty = "dotted", col = "red2")
```


---
class: clear 

```{r qqnorm-skew-right, fig.width=6, fig.asp=0.618, out.width="70%"}
set.seed(101)
x <- rlnorm(100)
qqnorm(x, main = "Skew right data")
qqline(x, lty = "dotted", col = "red2")
```


---
class: clear 

```{r qqnorm-skew-left, fig.width=6, fig.asp=0.618, out.width="70%"}
set.seed(101)
x <- -rlnorm(100)
qqnorm(x, main = "Skew left data")
qqline(x, lty = "dotted", col = "red2")
```


---
class: clear 

```{r qqnorm-heavy tails, fig.width=6, fig.asp=0.618, out.width="70%"}
set.seed(101)
x <- rt(100, df = 1)
qqnorm(x, main = "Heavy-tailed data")
qqline(x, lty = "dotted", col = "red2")
```


---
class: clear, middle, center

.larger[When should you use normality tests?]

--

```{r never, echo=FALSE, out.width="50%"}
my_id <- "1551b314-5e8a-4477-aca2-088c05963111-v1"
set.seed(4); RBitmoji::plot_comic(my_id, tag = "never")
```


---
class: clear, middle

.large[

```{r normality-tests}
# Shapiro-Wilk test and sample size
set.seed(101)  # for reproducibility
x <- replicate(100, c(
  shapiro.test(rt(10, df = 40))$p.value,
  shapiro.test(rt(100, df = 40))$p.value,
  shapiro.test(rt(1000, df = 40))$p.value,
  shapiro.test(rt(5000, df = 40))$p.value
))
rownames(x) <- c("n=10", "n=100", "n=1000", "n=5000")
rowMeans(x < 0.05)
```

]


---
class: clear, middle, center

```{r normal-vs-t, echo=FALSE, fig.width=6, fig.asp=0.618, out.width="90%"}
x <- seq(from = -5, to = 5, length = 500)
y1 <- dnorm(x)
y2 <- dt(x, df = 40)
df <- data.frame(
  x = c(x, x), 
  y = c(y1, y2),
  Distribution = rep(c("Standard\nnormal", "t(df = 50)"), each = length(x))
)
set.seed(2); comic <- RBitmoji::get_comic(my_id, tag = "fuck")
ggplot(df, aes(x = x, y = y, color = Distribution)) +
  geom_line(size = 1, alpha = 0.5) +
  labs(x = expression(x), y = "Density") +
  annotation_raster(comic, xmin = -5, xmax = -1.25, ymin = 0.25, ymax = 0.4)
```


---
class: clean, middle, center, inverse

.huge[Scaling the residuals]


---

# Standardized residuals

.large[

$$r_i^{stan} = \frac{r_i}{\sqrt{Var\left(r_i\right)}}$$
]

--

.large[

* The $r_i^{stan}$s have mean zero and variance .bold[approximately equal to one]

]

--

.large[

* Large positive/negative values of $r_i^{stan}$, say $\left|r_i^{stan}\right| > 3$ may indicate an outlier

]

--

.large[.center[.content-box-blue[

So what is $Var\left(r_i\right)$? `r emo::ji("thinking")`

]]]


---

# Standardized residuals

.large[

* If $\epsilon_i \stackrel{iid}{\sim} N\left(0, \sigma^2\right)$, then $\epsilon_i/\sigma \stackrel{iid}{\sim} N\left(0, 1\right)$, so it seems intuitive to define a standardized residual as $$r_i^{stan} =\frac{r_i}{\sqrt{MSE}} = \frac{r_i}{RMSE}$$

]

--

.large[

However, $Var\left(r_i\right) \ne MSE$!! 

]

```{r say-what, echo=FALSE, out.width="20%"}
set.seed(4); RBitmoji::plot_comic(my_id, tag = "say what")
```


---

# Standardized residuals

.large[

* Recall that $r_i = Y_i - \widehat{Y}_i$, or, in matrix form $$\boldsymbol{r} = \boldsymbol{Y} - \widehat{\boldsymbol{Y}}$$

]

--

.large[

* It can be shown that the variance of $r_i$ is given by $\sigma^2\left(1 - h_{ii}\right)$, which can be estimated with $MSE\left(1 - h_{ii}\right)$

]

--

.large[

* .purple[Studentized residual]: $r_i^{stud} = \frac{r_i}{\sqrt{MSE\left(1 - h_{ii}\right)}}$

    - Variance of residuals depende on $\boldsymbol{X}$

    - More effective at detecting outlying response values

]


---
class: clear, middle, center

```{r residual-patterms, echo=FALSE, out.width="70%"}
knitr::include_graphics("images/residual-patterns.png")
```


---

# Delivery data example `r emo::ji("truck")`

.medium[

```{r delivery-01}
# Load the delivery data
url <- "https://bgreenwell.github.io/uc-bana7052/data/delivery.csv"
delivery <- read.csv(url)
head(delivery, n = 10)  # print first 10 observations
```

]


---

# Delivery data example `r emo::ji("truck")`

.medium[

```{r delivery-02}
# Fit an MLR model
delivery_fit <- lm(DeliveryTime ~ NumberofCases + Distance, 
                   data = delivery)

# Fitted values
yhat <- fitted(delivery_fit)
# equivalent to `yhat <- predict(delivery_fit)`  #<<

# Residuals
r <- residuals(delivery_fit)
rstan <- rstandard(delivery_fit)
rstud <- rstudent(delivery_fit)
press <- rstandard(delivery_fit, type = "predictive")
```

]

---
class: clear, middle, center

```{r quittin-time, echo=FALSE, out.width="60%"}
RBitmoji::plot_comic(my_id, tag = "quittin")
```