---
title: "Residual Diagnostics and Remedial Measures"
subtitle: "Lecture 04"
author: "Brandon M. Greenwell"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  xaringan::moon_reader:
    mathjax: "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_HTMLorMML"
    css: [default, metropolis, metropolis-fonts, hygge, "custom.css"]
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---
class: clear, middle

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE, servr.daemon = TRUE)

# Global chunk options
knitr::opts_chunk$set(
  cache = FALSE,
  echo = TRUE,
  dev = "svglite",
  fig.align = "center",
  # fig.width = 6,
  # fig.asp = 0.618,
  # out.width = "70%",
  message = FALSE,
  warning = FALSE,
  error = FALSE
)

# Bitmoji id
my_id <- "1551b314-5e8a-4477-aca2-088c05963111-v1"

# Load required packages
library(ggplot2)
```

.large[

* Required reading

    - Chapters: 3, 6, and 10

    - Sections: 3.1-3.4, 3.7, 6.8, and 10.1-10.4

* Main topics:

    - Diagnostic and Remedial Measures (3.1-3.4, 3.7, 6.8,)
  
    - Model Adequacy and Outlying Observations (10.1-10.4)

]


---

# Prerquisites

.scrollable[

.medium[

```{r prerequisites, eval=FALSE}
# List of required (CRAN) packages
pkgs <- c(
  NULL
)

# Install required (CRAN) packages
for (pkg in pkgs) {
  if (!requireNamespace(pkg)) {  # check if already installed
    install.packages(pkg)  # install it
  }
}

# Install additional (optional) awesomeness
install.packages(c("devtools", "magick"))
devtools::install_github("bgreenwell/roundhouse")
```

]

]


---
class: clear, center, middle

```{r lets-go, echo=FALSE, out.width="70%"}
set.seed(4); RBitmoji::plot_comic(my_id, tag = "lets go")
```


---
class: clear, center, middle, inverse

.huger[

What are *residual diagnostics* and *remedial measures* and why do we need them?

]


---
class: clear, center, middle, inverse

.huger[

What's wrong with my model and how do I fix it?

]


---
class: clear 

.large[.bold[.center[.green[What are the typical assumptions of the linear regression model?]]]]

.large[

$$Y_i = \beta_0 + \beta_1 X_{i1} + \dots + \beta_{p-1} X_{i, p-1} + \epsilon_i\\i = 1, 2, \dots, n$$

]

--

.large[

* Independent observations: $Cov\left(\epsilon_i, \epsilon_j\right) = 0$ $\left(i \ne j\right)$

]

--

.large[

* Constant variance: $Var\left(\epsilon_i\right) = Cov\left(\epsilon_i, \epsilon_i\right) = \sigma^2$

]

--

.large[

* Normally distributed errors: $\epsilon_i \stackrel{iid}{\sim} N\left(0, \sigma^2\right)$ (.red[required for **traditional** statistical inference])

]

--

.large[

* We assume that we have the .bold[.darkorange[correct model]]!

]


---
class: clear, center, middle, inverse

.huger[

How do we know if the model is ~~wrong~~ bad?

]


---
class: clear, center, middle

.huger[What is a residual?]

--

.huge[ 

$e_i = Y_i - \widehat{Y}_i$ 

.red[observed error = data - fit]

]


---

# What can they tell us?

.medium[

* Plot of residuals against predictor variable (checking non-linearity).

* Plot of absolute or squared residuals against predictor variable (check non-constant variance)

* Plot of residuals against fitted values (non-constant variance and non-linearity)

* Plot of residuals against time or another sequence (non-independence)

* Plot of residuals against omitted predictor variables (missing potentially important predictors)

* Boxplot of residuals (outlying observations)

* Normal probability plot of residuals (non-normality).

]


---
class: clear, middle, center

```{r example-diagnostics, echo=FALSE, fig.width=6, fig.asp=1, out.width="70%"}
# Simualated data sets
set.seed(101)
n <- 100
df1 <- tibble::tibble(
  x = runif(n, min = -5, max = 5),
  y = 1 + 2*x^2 + rnorm(n, sd = 2)
)
df2 <- tibble::tibble(
  x = runif(n, min = 1, max = 10),
  y = 1 + 4*x + rnorm(n, sd = 2*x)
)
df2 <- rbind(df2, data.frame(x = 2, y = 50))
df3 <- tibble::tibble(
  x = runif(n, min = 1, max = 10),
  y = 1 + 4*x + arima.sim(list(order = c(1,0,0), ar = 0.7), n = n, sd = 20)
)
df4 <- tibble::tibble(
  x = runif(n, min = 1, max = 10),
  y = 1 + 4*x + rlnorm(n, sd = 1)
)

# Fitted models
fit1 <- lm(y ~ x, data = df1)
fit2 <- lm(y ~ x, data = df2)
fit3 <- lm(y ~ x, data = df3)
fit4 <- lm(y ~ x, data = df4)

# Residuals
r1 <- residuals(fit1)
r2 <- rstandard(fit2)
r3 <- residuals(fit3)
r4 <- residuals(fit4)

# Residual plots
par(mfrow = c(2, 2))
plot(df1$x, r1, xlab = "X", ylab = "Residual", 
     main = "Misspecified mean structure")
abline(h = 0, lty = "dotted", col = "red2")
plot(df2$x, r2, xlab = "X", ylab = "Residual",
     main = "Non-constant variance")
points(df2$x[101L], r2[101L], pch = 19, col = "red2")
abline(h = 0, lty = "dotted", col = "red2")
plot(residuals(fit3), xlab = "Index", ylab = "Residual", type = "b",
     main = "Serial correlation")
abline(h = 0, lty = "dotted", col = "red2")
qqnorm(r4, main = "Non-normal errors")
qqline(r4, lty = "dotted", col = "red2")
```


---

# Residual analysis

.large[

* Residuals help in identifying a misspecified mean structure (i.e., $\boldsymbol{\beta}\boldsymbol{X}$)

* *Scaled residuals* help in identifying outliers or extreme values

* Common types of residuals:

    - Standardized Residuals
    
    - Studentized residuals
    
    - PRESS Residuals 

]


---
class: clear, middle, center

.huge[Normal Q-Q plot can be used to asses the "normalityness" of a set of observations]


---
class: clear 

```{r qqnorm-normal, fig.width=6, fig.asp=0.618, out.width="70%"}
set.seed(101)
x <- rnorm(100)
qqnorm(x, main = "Normal data")
qqline(x, lty = "dotted", col = "red2")
```


---
class: clear 

```{r qqnorm-skew-right, fig.width=6, fig.asp=0.618, out.width="70%"}
set.seed(101)
x <- rlnorm(100)
qqnorm(x, main = "Skew right data")
qqline(x, lty = "dotted", col = "red2")
```


---
class: clear 

```{r qqnorm-skew-left, fig.width=6, fig.asp=0.618, out.width="70%"}
set.seed(101)
x <- -rlnorm(100)
qqnorm(x, main = "Skew left data")
qqline(x, lty = "dotted", col = "red2")
```


---
class: clear 

```{r qqnorm-heavy tails, fig.width=6, fig.asp=0.618, out.width="70%"}
set.seed(101)
x <- rt(100, df = 1)
qqnorm(x, main = "Heavy-tailed data")
qqline(x, lty = "dotted", col = "red2")
```


---
class: clear, middle, center

.larger[When should you use normality tests?]

--

```{r never, echo=FALSE, out.width="50%"}
my_id <- "1551b314-5e8a-4477-aca2-088c05963111-v1"
set.seed(4); RBitmoji::plot_comic(my_id, tag = "never")
```


---
class: clear, middle

.large[

```{r normality-tests}
# Shapiro-Wilk test and sample size
set.seed(101)  # for reproducibility
x <- replicate(100, c(
  shapiro.test(rt(10, df = 40))$p.value,
  shapiro.test(rt(100, df = 40))$p.value,
  shapiro.test(rt(1000, df = 40))$p.value,
  shapiro.test(rt(5000, df = 40))$p.value
))
rownames(x) <- c("n=10", "n=100", "n=1000", "n=5000")
rowMeans(x < 0.05)
```

]


---
class: clear, middle, center

```{r normal-vs-t, echo=FALSE, fig.width=6, fig.asp=0.618, out.width="90%"}
x <- seq(from = -5, to = 5, length = 500)
y1 <- dnorm(x)
y2 <- dt(x, df = 40)
df <- data.frame(
  x = c(x, x), 
  y = c(y1, y2),
  Distribution = rep(c("Standard\nnormal", "t(df = 50)"), each = length(x))
)
set.seed(2); comic <- RBitmoji::get_comic(my_id, tag = "fuck")
ggplot(df, aes(x = x, y = y, color = Distribution)) +
  geom_line(size = 1, alpha = 0.5) +
  labs(x = expression(x), y = "Density") +
  annotation_raster(comic, xmin = -5, xmax = -1.25, ymin = 0.25, ymax = 0.4)
```


---
class: clean, middle, center, inverse

.huge[Scaling the residuals]


---

# Standardized residuals

.large[

$$r_i^{stan} = \frac{r_i}{\sqrt{Var\left(r_i\right)}}$$
]

--

.large[

* The $r_i^{stan}$s have mean zero and variance .bold[approximately equal to one]

]

--

.large[

* Large positive/negative values of $r_i^{stan}$, say $\left|r_i^{stan}\right| > 3$ may indicate an outlier

]

--

.large[.center[.content-box-blue[

So what is $Var\left(r_i\right)$? `r emo::ji("thinking")`

]]]


---

# Standardized residuals

.large[

* If $\epsilon_i \stackrel{iid}{\sim} N\left(0, \sigma^2\right)$, then $\epsilon_i/\sigma \stackrel{iid}{\sim} N\left(0, 1\right)$, so it seems intuitive to define a standardized residual as $$r_i^{stan} =\frac{r_i}{\sqrt{MSE}} = \frac{r_i}{RMSE}$$

]

--

.large[

However, $Var\left(r_i\right) \ne MSE$!! 

]

```{r say-what, echo=FALSE, out.width="20%"}
set.seed(4); RBitmoji::plot_comic(my_id, tag = "say what")
```


---

# Standardized residuals

.large[

* Recall that $r_i = Y_i - \widehat{Y}_i$, or, in matrix form $$\boldsymbol{r} = \boldsymbol{Y} - \widehat{\boldsymbol{Y}}$$

]

--

.large[

* It can be shown that the variance of $r_i$ is given by $\sigma^2\left(1 - h_{ii}\right)$, which can be estimated with $MSE\left(1 - h_{ii}\right)$

]

--

.large[

* .purple[Studentized residual]: $r_i^{stud} = \frac{r_i}{\sqrt{MSE\left(1 - h_{ii}\right)}}$

    - Variance of residuals depends on $\boldsymbol{X}$

    - More effective at detecting outlying response values

]


---

# PRESS residuals

.medium[

* Another refinement to make the residuals more effective at detecting outlying $Y$ observations is to measure the *i*-th residual $r_i = Y_i - \widehat{Y}_i$ when the fitted regression is based on all of the cases except the *i*-th one

    - Also referred to as the *deleted residuals* or *jackknife residuals*

    - A form of *leave one out cross-validation* (LOOCV)

* It can be shown that these residuals can be obtained without refitting the model $n$ times using $$d_i = \frac{r_i}{1 - h_{ii}}$$

* PRESS statistic: $PRESS = \sum_{i=1}^n d_i^2$ (.red[useful for model selection])

]


---
class: clear, middle, center

```{r residual-patterms, echo=FALSE, out.width="70%"}
knitr::include_graphics("images/residual-patterns.png")
```


---

# Delivery data example `r emo::ji("truck")`

.medium[

```{r delivery-01}
# Load the delivery data
url <- "https://bgreenwell.github.io/uc-bana7052/data/delivery.csv"
delivery <- read.csv(url)
head(delivery, n = 10)  # print first 10 observations
```

]


---

# Delivery data example `r emo::ji("truck")`

.medium[

```{r delivery-02}
# Fit an MLR model
delivery_fit <- lm(DeliveryTime ~ NumberofCases + Distance, 
                   data = delivery)

# Fitted values
yhat <- fitted(delivery_fit)
# equivalent to `yhat <- predict(delivery_fit)`  #<<

# Residuals
r <- residuals(delivery_fit)  # ordinary  #<<
rstan <- rstandard(delivery_fit)  # studentized  #<<
rstud <- rstudent(delivery_fit)  # studentized deleted  #<<
press <- rstandard(delivery_fit, type = "predictive")  # PRESS  #<<
```

]


---

class: clear, middle

.scrollable[

```{r rstandard}
getAnywhere("rstandard.lm")
```

]


---

class: clear, middle

.medium[

```{r rstudent}
getAnywhere("rstudent.lm")
```

]


---
class: clear, middle, center

```{r leverage-influence, echo=FALSE, fig.width=6, fig.asp=0.5, out.width="90%"}
set.seed(101)
df1 <- tibble::tibble(
  x = c(1:5, 20),
  y = 1 + 2*x + rnorm(length(x))
)
set.seed(101)
df2 <- tibble::tibble(
  x = c(1:5),
  y = 1 + 2*x + rnorm(length(x))
)
df2 <- rbind(df2, data.frame(x = 20, y = 5))
p1 <- ggplot(df1, aes(x, y)) +
  geom_point(size = 2) +
  geom_smooth(method = "lm", se = FALSE) +
  ggtitle("Pure leverage point") +
  theme_light()
p2 <- ggplot(df2, aes(x, y)) +
  geom_point(size = 2) +
  geom_smooth(method = "lm", se = FALSE) +
  ggtitle("Influential point") +
  theme_light()
gridExtra::grid.arrange(p1, p2, nrow = 1)
```


---
class: clear, middle, center

.huge[Such problems are easy to see when there is only a single predictor!]


---

# Remedial measures

.center[

What do we do once we've discovered inadequacies in the model?

]

--

* Misspecified mean structure: transformations and non-parametric algorithms (e.g., MARS or a decision tree), etc.

* Non-constant variance: transformations, *weighted least squares* (WLS), and *generalized linear models* (GLMs)

* Outliers and influential values: robust and resistant regression procedures

    - See `?MASS::rlm` and `?MASS::lqs` for some options

* Autocorrelation: time series modeling


---

# Your turn `r emo::ji("scream")`

.larger[

Return to the Boston housing example from the previous lecture. Fit an MLR model using `cmed` as the response and `lstat` and `rm` as the predictors. Check the adequacy of the fitted model using the methods discussed in this lecture. Does the model appear adequate? What assumptions appear to be violated? Are there outlying and/or influential values?

]


---
class: clear, middle, center

```{r boston-spm, echo=FALSE, out.width="60%"}
GGally::ggpairs(pdp::boston[, c("cmedv", "lstat", "rm")])
```


---
class: clear, middle

```{r boston-broom-01, echo=FALSE}
library(broom)
library(dplyr)
boston2 <- pdp::boston %>%
  lm(cmedv ~ lstat + rm, data = .) %>%
  augment() %>%
  mutate(row_num = 1:n())
```

.scrollable[

```{r boston-broom-02}
library(broom)
library(dplyr)
pdp::boston %>%
  lm(cmedv ~ lstat + rm, data = .) %>%
  broom::augment() %>%
  mutate(row_num = 1:n())
```

]


---
class: clear, middle, center

```{r boston-residual-plots-01, echo=FALSE, fig.width=6, fig.asp=0.618, out.width="80%"}
ggplot(boston2, aes(x = .fitted, y = .std.resid)) +
  geom_point(alpha = 0.3) +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red2") +
  geom_hline(yintercept = c(-2, 2), linetype = "dotted") +
  # geom_smooth(color = "forestgreen", alpha = 0.1, se = FALSE) +
  xlab("Fitted value") +
  ylab("Standardized residual") +
  theme_light()
```


---
class: clear, middle, center

```{r boston-residual-plots-02, echo=FALSE, fig.width=6, fig.asp=0.618, out.width="80%"}
ggplot(boston2, aes(sample = .std.resid)) +
  geom_qq(alpha = 0.3) +
  geom_qq_line(linetype = "dashed", color = "red2") +
  xlab("Theoretical quantile") +
  ylab("Sample quantile") +
  theme_light()
```


---
class: clear, middle, center

```{r boston-residual-plots-03, echo=FALSE, fig.width=6, fig.asp=0.618, out.width="80%"}
ggplot(boston2, aes(x = .fitted, y = .hat)) +
  geom_point(alpha = 0.3) +
  xlab("Fitted value") +
  ylab("Leverage (i.e., hat value)") +
  theme_light()
```


---
class: clear, middle, center

```{r boston-residual-plots-04, echo=FALSE, fig.width=6, fig.asp=0.618, out.width="80%"}
ggplot(boston2, aes(x = row_num, y = .std.resid)) +
  geom_point(alpha = 0.3) +
  geom_line() +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red2") +
  xlab("Index") +
  ylab("Standardized residual") +
  theme_light()
```


---
class: clear, middle, center

```{r boston-residual-plots-05, echo=FALSE, fig.width=6, fig.asp=0.618, out.width="80%"}
ggplot(boston2, aes(x = lstat, y = .std.resid)) +
  geom_point(alpha = 0.3) +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red2") +
  geom_hline(yintercept = c(-2, 2), linetype = "dotted") +
  geom_smooth(color = "forestgreen", alpha = 0.1, se = FALSE) +
  ylab("Standardized residual") +
  theme_light()
```


---
class: clear, middle, center

```{r boston-residual-plots-06, echo=FALSE, fig.width=6, fig.asp=0.618, out.width="80%"}
ggplot(boston2, aes(x = rm, y = .std.resid)) +
  geom_point(alpha = 0.3) +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red2") +
  geom_hline(yintercept = c(-2, 2), linetype = "dotted") +
  geom_smooth(color = "forestgreen", alpha = 0.1, se = FALSE) +
  ylab("Standardized residual") +
  theme_light()
```


---
class: clear, middle, center

.huge[Leverage values]


---
class: clear, middle, center

.huge[Multicollinearity]


---

# Multicolinearity

* In many (typically non-experimental) situations, the predictor variables tend to be correlated among themselves

* When this correlation is "high", *multicollinearity* is said to exist

* Multicollinearity does not, in general, prevent us from obtaining a good fit! It can, however, cause other issues:

    - **Multicollinearity can** cause some of the estimated coefficients to become unstable (i.e., high standard errors)
    
    - **Multicollinearity can** complicate the interpretation of the estimated coefficients (e.g., predicting crop yield from the amount of rainfall and hours of sunshine)
    
* A simple way to assess whether or not multicollinearity is present is to use *variance inflation factors* (VIFs)

    - VIFs are available from the R package [car](https://cran.r-project.org/package=car) as the next example illustrates


---

# Heart catheter example

.large[

A study was conducted and data collected to fit a regression model to predict the length of a catheter needed to pass from a major artery at the femoral region and moved into the heart for children (Weisberg 1980). For 12 children, the proper catheter length was determined by checking with a fluoroscope that the catheter tip had reached the right position. The goal is to determine a model where a childâ€™s height and weight could be used to predict the proper catheter length.

]

.huge[.center[

[heart.R](https://github.com/bgreenwell/stt7140-env/blob/master/code/heart.R)

]]


---

# Heart catheter example

.large[

```{r heart-01}
# Load required packages
library(car)     # for vif() function
library(plotly)  # for interactive plotting

# Load the data
url <- "https://bgreenwell.github.io/uc-bana7052/data/heart"
heart <- read.table(url, header = TRUE)
head(heart)  
```

]


---

# Heart catheter example

.scrollable[

```{r heart-02, out.width="70%"}
# Scatterplot matrix
pairs(heart)
```

]


---

# Heart catheter example

.larger[

```{r heart-03, out.width="70%"}
# Correlation matrix
(cor_mat <- cor(heart))
```

]


---

# Heart catheter example

.scrollable[

```{r heart-04}
# Interactive 3-D scatterplot
p <- plot_ly(heart, x = ~Height, y = ~Weight, z = ~Length) %>%
  add_markers() %>%
  layout(scene = list(xaxis = list(title = "Weight (pounds)"),
                      yaxis = list(title = "Height (inches)"),
                      zaxis = list(title = "Length (cm)")))
p
```

]


---

# Heart catheter example

.scrollable[

```{r heart-05}
# Fit a linear model
heart_fit <- lm(Length ~ Height + Weight, data = heart)
summary(heart_fit)
```

]


---

# Heart catheter example

.large[

```{r heart-06}
# Compute variance inflation factors
vif(heart_fit)  #<<
```

]

--

.large[

* Both variance inflation factors are greater than 10 `r emo::ji("scream")`

]

--

.large[.center[.content-box-blue[

What are some potential remedies?

]]]


---

# Your turn `r emo::ji("scream")`

.larger[

Return to the Boston housing example from the previous lecture. Fit an MLR model using `cmed` as the response and `lstat` and `rm` as the predictors. Does multicollinearity appear to be an issue here? Explain.

]


---

# Solution `r emo::ji("sunglasses")`

.large[

```{r boston-vif}
# Compute VIF scores for the Boston housing example
pdp::boston %>%
  lm(cmedv ~ lstat + rm, data = .) %>%
  vif()
```

]

--

.center[.huger[

`r emo::ji("thumbsup")`

]]


---

# Other useful diagnostics

.huge[

* DFFITS

* DFBETAS

* Cook's distance

]


---
class: clear, middle, center

```{r quittin-time, echo=FALSE, out.width="60%"}
RBitmoji::plot_comic(my_id, tag = "quittin")
```
